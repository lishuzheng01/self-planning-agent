# src/rag_engine.py
import os
import shutil
from typing import List
from dotenv import load_dotenv

# LangChain ç»„ä»¶
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings 

load_dotenv()

class RAGEngine:
    def __init__(self, vector_db_path="./output/chroma_db"):
        self.vector_db_path = vector_db_path
        self.vector_store = None
        
        api_key = os.getenv("SILICONFLOW_API_KEY")
        if not api_key:
            raise ValueError("âŒ æœªæ‰¾åˆ° SILICONFLOW_API_KEY")

        print("âš™ï¸ åˆå§‹åŒ– RAG å¼•æ“ (Cloud Embedding)...")
        
        # é…ç½® Embedding API
        # SiliconFlow å…¼å®¹ OpenAI æ¥å£è§„èŒƒ
        self.embedding_model = OpenAIEmbeddings(
            model="BAAI/bge-m3",                # æŒ‡å®šç¡…åŸºæµåŠ¨æ”¯æŒçš„ Embedding æ¨¡å‹
            openai_api_key=api_key,
            openai_api_base="https://api.siliconflow.cn/v1",
            check_embedding_ctx_length=False    # å…³é—­æœ¬åœ° Token æ£€æŸ¥
        )

    def ingest_data(self, data_dir: str):
        """
        è¯»å– ./data ç›®å½• -> åˆ‡ç‰‡ -> API å‘é‡åŒ– -> å­˜å…¥ ChromaDB
        """
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
            print(f"âš ï¸ ç›®å½• {data_dir} ä¸å­˜åœ¨ï¼Œå·²è‡ªåŠ¨åˆ›å»ºã€‚è¯·æ”¾å…¥ txt/pdf æ–‡ä»¶ã€‚")
            return

        print(f"ğŸ“‚ æ‰«ææ–‡æ¡£ç›®å½•: {data_dir}")
        
        # 1. åŠ è½½æ‰€æœ‰ txt æ–‡ä»¶ (æ ¹æ®éœ€è¦å¯åŠ  "*.pdf")
        loader = DirectoryLoader(data_dir, glob="**/*.txt", show_progress=True)
        try:
            docs = loader.load()
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            return

        if not docs:
            print("âš ï¸ ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£ã€‚")
            return

        print(f"   -> æ‰¾åˆ° {len(docs)} ä¸ªæ–‡ä»¶")

        # 2. æ–‡æœ¬åˆ‡ç‰‡ (Chunking)
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=800, 
            chunk_overlap=100
        )
        splits = text_splitter.split_documents(docs)
        print(f"   -> åˆ‡åˆ†ä¸º {len(splits)} ä¸ªæ–‡æœ¬å—")

        # 3. å‘é‡åŒ–å¹¶å­˜å‚¨ (è¿™ä¸€æ­¥ä¼šæ¶ˆè€— API Token)
        print("   -> æ­£åœ¨è°ƒç”¨ API ç”Ÿæˆå‘é‡ (è¯·ç¨å€™)...")
        
        # æ¸…ç†æ—§æ•°æ® (å¯é€‰)
        if os.path.exists(self.vector_db_path):
            try:
                shutil.rmtree(self.vector_db_path)
            except:
                pass 

        self.vector_store = Chroma.from_documents(
            documents=splits,
            embedding=self.embedding_model,
            persist_directory=self.vector_db_path
        )
        print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

    def query_knowledge_base(self, query: str, top_k: int = 5) -> List[str]:
        """
        æ ¹æ®é—®é¢˜æ£€ç´¢ç›¸å…³èµ„æ–™
        """
        if not self.vector_store:
            if os.path.exists(self.vector_db_path):
                self.vector_store = Chroma(
                    persist_directory=self.vector_db_path, 
                    embedding_function=self.embedding_model
                )
            else:
                return []

        # æ£€ç´¢æ—¶ä¹Ÿä¼šè‡ªåŠ¨è°ƒç”¨ API å°† query å‘é‡åŒ–
        results = self.vector_store.similarity_search(query, k=top_k)
        return [doc.page_content for doc in results]
