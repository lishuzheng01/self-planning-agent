# src/search_engine.py

import os
import time
import logging
import requests
import random
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
from urllib.parse import urljoin, urlparse
from typing import List, Dict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

class ImageSearcher:
    def __init__(self, download_timeout=15): # å¢åŠ è¶…æ—¶æ—¶é—´ä»¥é€‚åº”å¤§å›¾ä¸‹è½½
        self.timeout = download_timeout
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
        }

    def search_text(self, keyword: str, max_results: int = 5) -> List[Dict]:
        """
        è”ç½‘æœç´¢æ–‡æœ¬èµ„æ–™ (å¢å¼ºç‰ˆ)
        è·å–æ›´å¤šç»“æœä»¥ä¾›ç­›é€‰
        """
        logger.info(f"ğŸ” [Text Search] '{keyword}'")
        results = []
        try:
            with DDGS() as ddgs:
                gen_results = ddgs.text(
                    keywords=keyword, 
                    region="wt-wt", 
                    safesearch="off", 
                    timelimit="y", # é™åˆ¶ä¸€å¹´å†…ï¼Œä¿è¯æ—¶æ•ˆæ€§
                    max_results=max_results
                )
                results = list(gen_results)
        except Exception as e:
            logger.error(f"DDGS Text Search Error: {e}")
        return results

    def search_and_download(self, keyword: str, save_dir: str) -> str:
        """
        æœå›¾å¹¶ä¸‹è½½ (é«˜è´¨é‡ä¼˜å…ˆç­–ç•¥)
        1. ä¼˜å…ˆå°è¯• Large + Wide (é«˜æ¸…æ¨ªå›¾)
        2. å¤±è´¥åˆ™é™çº§ä¸º Medium (ä¸­ç­‰å›¾)
        3. éªŒè¯æ–‡ä»¶å¤§å° (>50KB) ç¡®ä¿æ¸…æ™°åº¦
        """
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
            
        # ç­–ç•¥ A: ä¼˜å…ˆå¯»æ‰¾é«˜æ¸…å¤§å›¾ (WallPaper/Large) + æ¨ªæ„å›¾ (Wide)
        logger.info(f"ğŸ¨ [Image Search] '{keyword}' (High Quality Mode)")
        hq_urls = self._fetch_image_urls_ddgs(keyword, size="Large", layout="Wide")
        
        # ç­–ç•¥ B: å¦‚æœé«˜æ¸…å›¾æ²¡ç»“æœï¼Œå°è¯•ä¸­ç­‰å°ºå¯¸
        if not hq_urls:
            logger.info("âš ï¸ é«˜æ¸…å›¾æœªæ‰¾åˆ°ï¼Œé™çº§ä¸ºæ™®é€šæœç´¢...")
            hq_urls = self._fetch_image_urls_ddgs(keyword, size="Medium", layout=None)
            
        # ç­–ç•¥ C: å¤‡ç”¨çˆ¬è™«
        if not hq_urls:
            logger.info("âš ï¸ DDGS å¤±æ•ˆï¼Œå¯ç”¨å¤‡ç”¨çˆ¬è™«...")
            hq_urls = self._fetch_image_urls_bing_backup(keyword)

        if not hq_urls:
            return None

        # éå†ä¸‹è½½ï¼Œç›´åˆ°æ‰¾åˆ°ä¸€å¼ è´¨é‡åˆæ ¼çš„å›¾ç‰‡
        for url in hq_urls:
            # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
            filename = f"{int(time.time())}_{random.randint(1000,9999)}.jpg"
            save_path = os.path.join(save_dir, filename)
            
            # ä¸‹è½½å¹¶éªŒè¯è´¨é‡
            if self._download_image(url, save_path, min_size_kb=50): # è‡³å°‘50KB
                return save_path
                
        return None

    def _fetch_image_urls_ddgs(self, keyword, size="Large", layout="Wide"):
        urls = []
        try:
            with DDGS() as ddgs:
                # size å‚æ•°: Small, Medium, Large, Wallpaper
                # layout å‚æ•°: Square, Tall, Wide
                results = ddgs.images(
                    keywords=keyword, 
                    region="wt-wt", 
                    safesearch="off", 
                    size=size, 
                    layout=layout,
                    max_results=10 # å¤šæŠ“å–ä¸€äº›ä¾›ç­›é€‰
                )
                urls = [r.get('image') for r in results if r.get('image')]
        except Exception as e:
            logger.warning(f"DDGS Image Search Error: {e}")
        return urls

    def _fetch_image_urls_bing_backup(self, keyword):
        """å¤‡ç”¨çˆ¬è™« (é€šå¸¸åªèƒ½è·å–åˆ°ä¸­ç­‰è´¨é‡)"""
        urls = []
        try:
            search_url = f"https://www.bing.com/images/search?q={keyword}&first=1"
            response = requests.get(search_url, headers=self.headers, timeout=5)
            soup = BeautifulSoup(response.text, 'lxml')
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src and src.startswith('http'):
                    urls.append(src)
        except Exception:
            pass
        return list(set(urls))[:15]

    def _download_image(self, url, save_path, min_size_kb=30):
        """
        ä¸‹è½½å¹¶æ‰§è¡Œä¸¥æ ¼çš„è´¨é‡æ£€æŸ¥
        :param min_size_kb: æœ€å°æ–‡ä»¶å¤§å° (KB)ï¼Œä½äºæ­¤å€¼è§†ä¸ºç¼©ç•¥å›¾/åå›¾
        """
        try:
            headers = self.headers.copy()
            parsed_url = urlparse(url)
            headers['Referer'] = f"{parsed_url.scheme}://{parsed_url.netloc}"
            
            response = requests.get(url, headers=headers, timeout=self.timeout)
            
            if response.status_code == 200:
                content = response.content
                file_size_kb = len(content) / 1024
                
                # 1. å¤§å°æ£€æŸ¥
                if file_size_kb < min_size_kb:
                    logger.warning(f"  -> è·³è¿‡è¿‡å°å›¾ç‰‡: {file_size_kb:.1f}KB < {min_size_kb}KB")
                    return False
                
                # 2. æ ¼å¼æ£€æŸ¥ (Magic Number)
                header = content[:4].hex().upper()
                if header.startswith("FFD8") or header.startswith("89504E47"): # JPG or PNG
                    with open(save_path, 'wb') as f:
                        f.write(content)
                    logger.info(f"  âœ… å›¾ç‰‡ä¸‹è½½æˆåŠŸ ({file_size_kb:.1f}KB): {save_path}")
                    return True
                else:
                    logger.warning(f"  -> æ ¼å¼ä¸æ”¯æŒ: {header}")
                    
        except Exception as e:
            logger.warning(f"  -> ä¸‹è½½å¼‚å¸¸: {str(e)[:50]}...")
            pass
        return False
