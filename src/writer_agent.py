# src/writer_agent.py

import os
import json
import re
import logging
from typing import List, Dict, Any

from src.llm_client import LLMClient
from src.rag_engine import RAGEngine
from src.search_engine import ImageSearcher

logging.basicConfig(level=logging.WARNING)
logger = logging.getLogger(__name__)

class WriterAgent:
    def __init__(self, output_dir="./output"):
        self.llm = LLMClient()
        self.output_dir = output_dir
        self.assets_dir = os.path.join(self.output_dir, "assets")
        os.makedirs(self.assets_dir, exist_ok=True)

        # ä»»åŠ¡éš”ç¦»ï¼šç¡®ä¿ RAG çº¯å‡€
        task_db_path = os.path.join(self.output_dir, "chroma_db")
        self.rag = RAGEngine(vector_db_path=task_db_path)
        
        self.searcher = ImageSearcher()
        
        # æ¨¡å‹é…ç½®
        self.model_planner = "deepseek-ai/DeepSeek-V3"
        self.model_writer = "Qwen/Qwen2.5-72B-Instruct" 
        self.model_visualizer = "Qwen/Qwen2.5-72B-Instruct"

    def plan_outline(self, topic: str) -> List[Dict]:
        """Step 1: ç”Ÿæˆå¤§çº² (å¢å¼ºç‰ˆ JSON ä¿®å¤)"""
        prompt = f"""
        ä½ æ˜¯ä¸€åä¸“ä¸šçš„æŠ€æœ¯ä¸»ç¼–ã€‚è¯·æ ¹æ®ä¸»é¢˜ "{topic}" è§„åˆ’ä¸€ç¯‡æ–‡ç« çš„å¤§çº²ã€‚
        
        ğŸ”´ **ä¸¥æ ¼æ ¼å¼è¦æ±‚**ï¼š
        1. å¿…é¡»è¿”å›ä¸€ä¸ªæ ‡å‡†çš„ **JSON æ•°ç»„** (Array of Objects)ã€‚
        2. **ç¦æ­¢**è¿”å›å­—å…¸æˆ–å¸¦ç´¢å¼•çš„å¯¹è±¡ï¼ˆå¦‚ {{"0": {{...}}}}ï¼‰ã€‚
        3. ä¸è¦åŒ…å« Markdown æ ‡è®°ã€‚
        
        æ­£ç¡®æ ¼å¼ç¤ºä¾‹ï¼š
        [
            {{"title": "ç¬¬ä¸€ç« æ ‡é¢˜", "description": "æ‘˜è¦..."}},
            {{"title": "ç¬¬äºŒç« æ ‡é¢˜", "description": "æ‘˜è¦..."}}
        ]
        """
        response = self.llm.call_llm(prompt, self.model_planner, json_mode=True)
        
        try:
            # 1. åŸºç¡€æ¸…æ´—
            clean_json = response.replace("```json", "").replace("```", "").strip()
            
            # 2. å°è¯•ç›´æ¥è§£æ
            try:
                data = json.loads(clean_json)
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤å¸¸è§é”™è¯¯ï¼ˆå¦‚é”®åæœªåŠ å¼•å·ï¼‰
                # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªç®€å•çš„æ­£åˆ™æå–ç­–ç•¥ä½œä¸ºå…œåº•
                # æå–æ‰€æœ‰ {"title": ..., "description": ...} ç»“æ„
                pattern = r'\{\s*"title":\s*".*?",\s*"description":\s*".*?"\s*\}'
                matches = re.findall(pattern, clean_json, re.DOTALL)
                if matches:
                    data = [json.loads(m) for m in matches]
                else:
                    return []

            # 3. ã€æ ¸å¿ƒä¿®å¤ã€‘ç»“æ„æ ‡å‡†åŒ– (Dict è½¬ List)
            # è§£å†³ 0: {...}, 1: {...} è¿™ç§å­—å…¸æ ¼å¼
            outline = []
            if isinstance(data, list):
                outline = data
            elif isinstance(data, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œå¯èƒ½æ˜¯ {"outline": [...]} æˆ– {"0": {...}, "1": {...}}
                # ç­–ç•¥ï¼šä¼˜å…ˆæ‰¾ list ç±»å‹çš„ valueï¼Œæ‰¾ä¸åˆ°åˆ™å–æ‰€æœ‰ dict ç±»å‹çš„ value
                
                # æƒ…å†µ A: {"chapters": [ ... ]}
                for key, val in data.items():
                    if isinstance(val, list):
                        outline = val
                        break
                
                # æƒ…å†µ B: {"0": {...}, "1": {...}}
                if not outline:
                    # æŒ‰ key æ’åºåå– value
                    sorted_keys = sorted(data.keys(), key=lambda x: int(str(x)) if str(x).isdigit() else x)
                    for k in sorted_keys:
                        if isinstance(data[k], dict):
                            outline.append(data[k])
            
            return outline
            
        except Exception as e:
            logger.error(f"å¤§çº²è§£æä¸¥é‡é”™è¯¯: {e}")
            return []

    def write_single_section(self, topic: str, section: Dict, index: int) -> Dict[str, Any]:
        """
        Step 2: æ’°å†™å•ç«  (å…ˆç”Ÿæˆæœç´¢è¯ -> å†æœç´¢ -> å†å†™ä½œ)
        """
        title = section.get('title', f'Section {index}')
        desc = section.get('description', '')
        
        # --- 1. æ™ºèƒ½ç”Ÿæˆæœç´¢å…³é”®è¯ (é¿å…æœä¸åˆ°å†…å®¹) ---
        # ä»¥å‰æ˜¯ f"{topic} {title}"ï¼Œç°åœ¨è®© LLM å˜é€šä¸€ä¸‹
        search_queries = self._generate_search_queries(topic, title, desc)
        
        # --- 2. æ··åˆæ£€ç´¢ ---
        
        # A. æœ¬åœ° RAG
        rag_query = f"{topic} {title} {desc}"
        rag_results = self.rag.query_knowledge_base(rag_query, top_k=2)
        
        # B. äº’è”ç½‘æœç´¢ (å¤šè¯å°è¯•)
        web_results = []
        seen_urls = set()
        
        # å¯¹ç”Ÿæˆçš„æ¯ä¸ªæœç´¢è¯éƒ½è¯•ä¸€ä¸‹
        for query in search_queries:
            results = self.searcher.search_text(query, max_results=2)
            for r in results:
                if r['href'] not in seen_urls:
                    web_results.append(r)
                    seen_urls.add(r['href'])
        
        # é™åˆ¶æ•°é‡ï¼Œé˜²æ­¢ä¸Šä¸‹æ–‡æº¢å‡º
        web_results = web_results[:4]
        
        # --- 3. æ„é€ ä¸Šä¸‹æ–‡ ---
        context_parts = []
        
        if rag_results:
            context_parts.append("ã€æœ¬åœ°æ–‡ä»¶èµ„æ–™ (Priority High)ã€‘ï¼š")
            for idx, txt in enumerate(rag_results):
                context_parts.append(f"[Local-{idx+1}] {txt[:400]}...")

        if web_results:
            context_parts.append("ã€äº’è”ç½‘æœ€æ–°èµ„è®¯ (Priority Medium)ã€‘ï¼š")
            for w in web_results:
                context_parts.append(f"æ¥æº: [{w['title']}]({w['href']})\nå†…å®¹æ‘˜è¦: {w['body']}")
        
        if not context_parts:
            context_parts.append("ï¼ˆæš‚æ— ç›´æ¥å‚è€ƒèµ„æ–™ï¼Œè¯·åŸºäºæ‚¨çš„ä¸“ä¸šçŸ¥è¯†æ’°å†™ã€‚ï¼‰")
            
        full_context_str = "\n\n".join(context_parts)
        
        # --- 4. å†™ä½œ ---
        content = self._generate_text_with_citation(topic, title, desc, full_context_str)
        
        # --- 5. é…å›¾ ---
        img_md, img_path, keyword = self._auto_append_image(content)
        
        full_md = f"## {title}\n\n{content}\n\n{img_md}\n\n---\n\n"

        return {
            "title": title,
            "markdown": full_md,
            "pure_text": content,
            "rag_context": rag_results, 
            "web_context": web_results,
            "search_queries": search_queries, # è¿”å›æœç´¢è¯ä¾› UI å±•ç¤º
            "search_keyword": keyword, 
            "image_path": img_path
        }

    def _generate_search_queries(self, topic, title, desc) -> List[str]:
        """è®© LLM å°†ç« èŠ‚æ„å›¾è½¬åŒ–ä¸º 2-3 ä¸ªæœç´¢å¼•æ“å‹å¥½çš„å…³é”®è¯"""
        prompt = f"""
        è¯·å°†ç« èŠ‚å†…å®¹è½¬åŒ–ä¸º 2 ä¸ªäº’è”ç½‘æœç´¢æŸ¥è¯¢è¯ã€‚
        ä¸»é¢˜ï¼š{topic}
        ç« èŠ‚ï¼š{title} ({desc})
        
        è¦æ±‚ï¼š
        1. ä¸€ä¸ªå®½æ³›è¯ (ä¾‹å¦‚: "{topic} latest news")
        2. ä¸€ä¸ªç²¾å‡†è¯ (ä¾‹å¦‚: "{title} data analysis")
        3. ç›´æ¥è¿”å›å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¸è¦è§£é‡Šã€‚
        """
        response = self.llm.call_llm(prompt, self.model_planner)
        # æ¸…æ´—
        queries = [q.strip() for q in response.split(',') if q.strip()]
        # å…œåº•
        if not queries:
            queries = [f"{topic} {title}"]
        return queries[:2]

    def _generate_text_with_citation(self, topic, title, desc, context) -> str:
        prompt = f"""
        ä½ æ˜¯ä¸€åä¸¥è°¨çš„æŠ€æœ¯ä½œå®¶ã€‚è¯·æ ¹æ®ã€å‚è€ƒèµ„æ–™ã€‘æ’°å†™æ–‡ç« ç« èŠ‚ã€‚
        ã€æ–‡ç« ä¸»é¢˜ã€‘ï¼š{topic}
        ã€æœ¬ç« æ ‡é¢˜ã€‘ï¼š{title}
        ã€æœ¬ç« æ‘˜è¦ã€‘ï¼š{desc}
        ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
        {context}
        
        ğŸ”´ **å…³é”®è¦æ±‚**ï¼š
        1. **åŸºäºäº‹å®**ï¼šå†…å®¹å¿…é¡»ä¼˜å…ˆåŸºäºæä¾›çš„ã€å‚è€ƒèµ„æ–™ã€‘ã€‚
        2. **ç¦æ­¢æ— å…³å¼•ç”¨**ï¼šç»å¯¹ä¸è¦å¼•ç”¨ä¸æœ¬ç« æ— å…³çš„é“¾æ¥ã€‚
        3. **æ ‡æ³¨å¼•ç”¨**ï¼šå½“ä½ å¼•ç”¨ã€äº’è”ç½‘æœ€æ–°èµ„è®¯ã€‘ä¸­çš„æ•°æ®æˆ–è§‚ç‚¹æ—¶ï¼Œå¿…é¡»åœ¨å¥æœ«åŠ ä¸Š Markdown é“¾æ¥å¼•ç”¨ã€‚
           - æ ¼å¼ï¼š`...è§‚ç‚¹æè¿° [æ¥æºæ ‡é¢˜](URL)`
        4. **æ·±åº¦ä¸é€»è¾‘**ï¼šç»¼åˆåˆ†æï¼Œä¸è¦ç½—åˆ—ã€‚
        5. **å­—æ•°**ï¼š400-600å­—ã€‚
        """
        return self.llm.call_llm(prompt, self.model_writer)

    def _auto_append_image(self, text_content: str):
        prompt = f"""
        é˜…è¯»ä»¥ä¸‹æ–‡æœ¬ï¼Œæå–ä¸€ä¸ªæœ€é€‚åˆåšæ’å›¾çš„â€œè‹±æ–‡æœç´¢å…³é”®è¯â€ã€‚
        æ–‡æœ¬ï¼š{text_content[:300]}...
        è¦æ±‚ï¼šåªè¿”å›å…³é”®è¯ï¼Œä¸è¦è§£é‡Šã€‚å¿…é¡»æ˜¯è‹±æ–‡ã€‚
        """
        keyword = self.llm.call_llm(prompt, self.model_visualizer).strip()
        keyword = re.sub(r'[^a-zA-Z0-9\s]', '', keyword)
        
        if not keyword:
            return "", None, ""
            
        image_path = self.searcher.search_and_download(keyword, self.assets_dir)
        
        if image_path:
            rel_path = os.path.relpath(image_path, self.output_dir).replace("\\", "/")
            return f"![å›¾ï¼š{keyword}]({rel_path})", image_path, keyword
        else:
            return f"> *(é…å›¾å¤±è´¥: {keyword})*", None, keyword
